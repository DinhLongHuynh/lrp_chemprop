{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.utils.data import IterableDataset\n",
    "from chemprop import data\n",
    "from chemprop import data, featurizers\n",
    "from chemprop.data.collate import collate_batch\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook illustrates the use of torch.utils.data.IterableDataset in order to sequentially load the dataset and handle it. \n",
    "\n",
    "**Context:** I want to train a ChemProp model using a dataset of 1 million compounds. While this is not an excessively large dataset, my MacBook M1 with 8GB of RAM struggles to process the entire CSV file into MolecularDatapoints. The system works fine with the CSV file, but struggles with the MolecularDatapoints. As a result, I am looking for an alternative approach to load small subsets of the CSV file sequentially, generate MolecularDatapoints, then create a Dataset and DataLoader, and finally train the model. One of the challenges I face is ensuring the data is shuffled after completing each training epoch. To address this, I found that `torch.utils.data.IterableDataset` is a useful class for my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by creating some useful functions to prepare the Chemprop dataset, as outlined in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_preparator(df,smiles_column,target_column):\n",
    "    smis = df.loc[:,smiles_column].values\n",
    "    ys = df.loc[:,[target_column]].values\n",
    "            \n",
    "    datapoints = [data.MoleculeDatapoint.from_smi(smi,y) for smi, y in zip(smis,ys)]\n",
    "    return datapoints\n",
    "\n",
    "\n",
    "def dataset_preparator(df, smiles_column, target_column, featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()):\n",
    "    datapoints = datapoint_preparator(df=df, smiles_column=smiles_column, target_column=target_column)\n",
    "    dataset = data.MoleculeDataset(datapoints, featurizer=featurizer)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAIN PART: StreamingMolDataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMolDatapoints(IterableDataset):\n",
    "    '''A class to prepare data for streaming, which is a subclass of IterableDataset. \n",
    "    The output is a generator that yields one chemprop.data.datasets.Datum at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df, smiles_column, target_column, scaler = None, size_at_time=100, shuffle=True):\n",
    "        '''Parameters:\n",
    "        ----------\n",
    "        df (pd.DataFrame): A pandas dataframe containing the data.\n",
    "        smiles_column (str): The column name containing SMILES strings.\n",
    "        target_column (str): The column name containing the target values.\n",
    "        scaler (StandardScaler): A StandardScaler object (already fitted) for normalizing the target values.\n",
    "        size_at_time (int): The number of samples to transfrom into chemprop.data.datasets.Datum at a time.\n",
    "        shuffle (boolean): If the df is shuffled.'''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.size_at_time = size_at_time\n",
    "        self.shuffle= shuffle\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''A function to define iteration logic. It take the whole csv data, then shuffled, then access to only a subset of data at a time for transformation.\n",
    "        The output is a generator that yields chemprop.data.datasets.Datum and ready to put through DataLoader.\n",
    "        '''\n",
    "\n",
    "        if self.shuffle:\n",
    "            df_shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            df_shuffled = self.df.copy()\n",
    "\n",
    "        # Transform pandas dataframe to molecule dataset according to size_at_time, prevent overloading memory. This is to balance between memory and speed.\n",
    "        for i in range(0, len(df_shuffled), self.size_at_time):\n",
    "            df_at_time = df_shuffled.iloc[i:i + self.size_at_time]\n",
    "            df_process = dataset_preparator(df=df_at_time, smiles_column=self.smiles_column, target_column=self.target_column)\n",
    "\n",
    "            if self.scaler != None: \n",
    "                df_process.normalize_targets(self.scaler)\n",
    "        \n",
    "            for mol in df_process: \n",
    "                yield mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test 1: Memory usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data_path = 'on_the_fly_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.sample(100000)\n",
    "scaler = StandardScaler().fit(df[[target_column]])\n",
    "\n",
    "# Function to record memory\n",
    "def memory_record():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / 1024 ** 2  # in MB\n",
    "    return mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage to load streaming dataset: 0.0 MB \n",
      "Time to load streaming dataset: 0.00018095970153808594 s \n"
     ]
    }
   ],
   "source": [
    "gc.collect() \n",
    "start_time = time.time()\n",
    "memory_before = memory_record()\n",
    "iterable_dataset = IterableMolDatapoints(\n",
    "    df=df,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    size_at_time=100, scaler=None, shuffle=True\n",
    ")\n",
    "memory_after =memory_record()\n",
    "end_time = time.time()\n",
    "gc.collect() \n",
    "\n",
    "print(f'Memory usage to load streaming dataset: {memory_after-memory_before} MB ')\n",
    "print(f'Time to load streaming dataset: {end_time-start_time} s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage to load chemprop dataset: -226.921875 MB \n",
      "Time to load streaming dataset: 10.86172890663147 s \n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "start_time = time.time()\n",
    "memory_before = memory_record()\n",
    "dataset = dataset_preparator(\n",
    "    df=df,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column\n",
    ")\n",
    "memory_after = memory_record()\n",
    "end_time = time.time()\n",
    "gc.collect()\n",
    "\n",
    "print(f'Memory usage to load chemprop dataset: {memory_after-memory_before} MB ')\n",
    "print(f'Time to load streaming dataset: {end_time-start_time} s ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test 2: Similarity to chemprop data loader**\n",
    "\n",
    "In this test, I aim to demonstrate that the function works similarly to the Chemprop data loader. Additionally, we can apply a scaler if necessary; however, it is important to fit the scaler on the entire dataset (Pandas DataFrame) before applying it.\n",
    "\n",
    "For illustration purposes, I will only take 10 instances from the whole dataset for examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train with unscaled target values: \n",
      "13632    -6.81384\n",
      "49724    -7.72072\n",
      "96557    -5.96453\n",
      "47916    -6.02439\n",
      "194343   -5.28958\n",
      "100979   -6.23839\n",
      "85015    -5.51530\n",
      "117896   -7.26316\n",
      "56923    -7.54427\n",
      "21348    -7.18239\n",
      "Name: docking_score, dtype: float64\n",
      "----------------------------------------\n",
      "df_train with scaled target values: \n",
      "0   -0.200613\n",
      "1   -1.063134\n",
      "2    0.607155\n",
      "3    0.550223\n",
      "4    1.249091\n",
      "5    0.346690\n",
      "6    1.034412\n",
      "7   -0.627955\n",
      "8   -0.895315\n",
      "9   -0.551136\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "\n",
    "df_train = pd.read_csv('on_the_fly_data.csv')\n",
    "df_train_10 = df_train.sample(10)\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "\n",
    "print(f'df_train with unscaled target values: \\n{df_train_10.docking_score}')\n",
    "print('-'*40)\n",
    "print(f'df_train with scaled target values: \\n{pd.Series(scaler.transform(df_train_10[[target_column]]).reshape(-1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chemprop Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with Chemprop data loader\n",
      "Batch 1\n",
      "tensor([[-6.8138],\n",
      "        [-7.7207],\n",
      "        [-5.9645],\n",
      "        [-6.0244],\n",
      "        [-5.2896]])\n",
      "Batch 2\n",
      "tensor([[-6.2384],\n",
      "        [-5.5153],\n",
      "        [-7.2632],\n",
      "        [-7.5443],\n",
      "        [-7.1824]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-6.8138],\n",
      "        [-7.7207],\n",
      "        [-5.9645],\n",
      "        [-6.0244],\n",
      "        [-5.2896]])\n",
      "Batch 2\n",
      "tensor([[-6.2384],\n",
      "        [-5.5153],\n",
      "        [-7.2632],\n",
      "        [-7.5443],\n",
      "        [-7.1824]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_preparator(df_train_10, smiles_column, target_column)\n",
    "train_loader = data.build_dataloader(train_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "print('Data batches with Chemprop data loader')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with StreamingMolDataset:\n",
      "Batch 1\n",
      "tensor([[-6.8138],\n",
      "        [-7.7207],\n",
      "        [-5.9645],\n",
      "        [-6.0244],\n",
      "        [-5.2896]])\n",
      "Batch 2\n",
      "tensor([[-6.2384],\n",
      "        [-5.5153],\n",
      "        [-7.2632],\n",
      "        [-7.5443],\n",
      "        [-7.1824]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-6.8138],\n",
      "        [-7.7207],\n",
      "        [-5.9645],\n",
      "        [-6.0244],\n",
      "        [-5.2896]])\n",
      "Batch 2\n",
      "tensor([[-6.2384],\n",
      "        [-5.5153],\n",
      "        [-7.2632],\n",
      "        [-7.5443],\n",
      "        [-7.1824]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = IterableMolDatapoints(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    size_at_time=5, scaler=None, shuffle=False\n",
    ")\n",
    "\n",
    "iterable_train_loader = data.build_dataloader(\n",
    "    iterable_dataset,\n",
    "    batch_size=5, shuffle=False)\n",
    "\n",
    "print('Data batches with StreamingMolDataset:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(iterable_train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point:** Without shuffling, the results indicated that the Chemprop dataset and the Streaming dataset behaved similarly. One advantage of the Streaming dataset is that it doesn't require generating all data points at once. \n",
    "\n",
    "Additionally, we can apply scaling to it, especially during training. However, this requires fitting an external scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Scaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with scaled target values:\n",
      "Batch 1\n",
      "tensor([[-0.2006],\n",
      "        [-1.0631],\n",
      "        [ 0.6072],\n",
      "        [ 0.5502],\n",
      "        [ 1.2491]])\n",
      "Batch 2\n",
      "tensor([[ 0.3467],\n",
      "        [ 1.0344],\n",
      "        [-0.6280],\n",
      "        [-0.8953],\n",
      "        [-0.5511]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-0.2006],\n",
      "        [-1.0631],\n",
      "        [ 0.6072],\n",
      "        [ 0.5502],\n",
      "        [ 1.2491]])\n",
      "Batch 2\n",
      "tensor([[ 0.3467],\n",
      "        [ 1.0344],\n",
      "        [-0.6280],\n",
      "        [-0.8953],\n",
      "        [-0.5511]])\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DataLoader with batch_size = 5 (2 batches) and scaled\n",
    "scaler = StandardScaler().fit(df_train[[target_column]]) # Fit on the whole train_data.\n",
    "\n",
    "iterable_dataset = IterableMolDatapoints(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    size_at_time=5, scaler=scaler, shuffle=False\n",
    ")\n",
    "\n",
    "iterable_train_loader = data.build_dataloader(\n",
    "    iterable_dataset,\n",
    "    batch_size=5, shuffle=False)\n",
    "\n",
    "print('Data batches with scaled target values:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(iterable_train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**\n",
    "\n",
    "In this part, when shuffle is activated, the samples in each batch are different between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with unscaled target values:\n",
      "Batch 1\n",
      "tensor([[-5.5153],\n",
      "        [-7.2632],\n",
      "        [-7.5443],\n",
      "        [-6.2384],\n",
      "        [-5.9645]])\n",
      "Batch 2\n",
      "tensor([[-5.2896],\n",
      "        [-6.0244],\n",
      "        [-6.8138],\n",
      "        [-7.1824],\n",
      "        [-7.7207]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-6.0244],\n",
      "        [-7.1824],\n",
      "        [-6.8138],\n",
      "        [-5.5153],\n",
      "        [-6.2384]])\n",
      "Batch 2\n",
      "tensor([[-7.5443],\n",
      "        [-7.2632],\n",
      "        [-5.2896],\n",
      "        [-5.9645],\n",
      "        [-7.7207]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = IterableMolDatapoints(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    size_at_time=5, scaler=None, shuffle=True\n",
    ")\n",
    "\n",
    "iterable_train_loader = data.build_dataloader(\n",
    "    iterable_dataset,\n",
    "    batch_size=5, shuffle=False)\n",
    "\n",
    "print('Data batches with unscaled target values:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(iterable_train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
