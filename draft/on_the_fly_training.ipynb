{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from chemprop import data\n",
    "from torch.utils.data import IterableDataset\n",
    "import torch\n",
    "from chemprop import data\n",
    "from chemprop import data, featurizers\n",
    "import math\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from chemprop.data.collate import collate_batch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook illustrates the use of torch.utils.data.IterableDataset in order to sequentially load the dataset and handle it. \n",
    "\n",
    "**Context:** I want to train a ChemProp model using a dataset of 1 million compounds. While this is not an excessively large dataset, my MacBook M1 with 8GB of RAM struggles to process the entire CSV file into MolecularDatapoints. The system works fine with the CSV file, but struggles with the MolecularDatapoints. As a result, I am looking for an alternative approach to load small subsets of the CSV file sequentially, generate MolecularDatapoints, then create a Dataset and DataLoader, and finally train the model. One of the challenges I face is ensuring the data is shuffled after completing each training epoch. To address this, I found that `torch.utils.data.IterableDataset` is a useful class for my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by creating some useful functions to prepare the Chemprop dataset, as outlined in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_preparator(df,smiles_column,target_column):\n",
    "    smis = df.loc[:,smiles_column].values\n",
    "    ys = df.loc[:,[target_column]].values\n",
    "            \n",
    "    datapoints = [data.MoleculeDatapoint.from_smi(smi,y) for smi, y in zip(smis,ys)]\n",
    "    return datapoints\n",
    "\n",
    "\n",
    "def dataset_preparator(df, smiles_column, target_column, featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()):\n",
    "    datapoints = datapoint_preparator(df=df, smiles_column=smiles_column, target_column=target_column)\n",
    "    dataset = data.MoleculeDataset(datapoints, featurizer=featurizer)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAIN PART: StreamingMolDataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingMolDataset(IterableDataset):\n",
    "    def __init__(self, df, smiles_column, target_column, scaler = None, batch_size=64, shuffle=True):\n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle= shuffle\n",
    "        self.scaler = scaler\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            df_shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            df_shuffled = self.df.copy()\n",
    "\n",
    "\n",
    "        for i in range(0, len(df_shuffled), self.batch_size):\n",
    "            df_batch = df_shuffled.iloc[i:i + self.batch_size]\n",
    "            df_process = dataset_preparator(df=df_batch, smiles_column=self.smiles_column, target_column=self.target_column)\n",
    "\n",
    "            if self.scaler != None: \n",
    "                df_process.normalize_targets(self.scaler)\n",
    "        \n",
    "        # Yield all the samples in the current batch\n",
    "            for mol in df_process: \n",
    "                yield mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test 1: Memory usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/drggnx1n5cv7mnctplfkwlzh0000gn/T/ipykernel_88974/3391832607.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "data_path = 'on_the_fly_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.sample(100000)\n",
    "scaler = StandardScaler().fit(df[[target_column]])\n",
    "batch_size=64\n",
    "\n",
    "# Function to record memory\n",
    "def memory_record():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / 1024 ** 2  # in MB\n",
    "    return mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage to load streaming dataset: 0.015625 MB \n",
      "Time to load streaming dataset: 0.0001621246337890625 s \n"
     ]
    }
   ],
   "source": [
    "gc.collect() \n",
    "start_time = time.time()\n",
    "memory_before = memory_record()\n",
    "streaming_dataset = StreamingMolDataset(\n",
    "    df=df,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    batch_size=batch_size, scaler=None, shuffle=True\n",
    ")\n",
    "memory_after =memory_record()\n",
    "end_time = time.time()\n",
    "gc.collect() \n",
    "\n",
    "print(f'Memory usage to load streaming dataset: {memory_after-memory_before} MB ')\n",
    "print(f'Time to load streaming dataset: {end_time-start_time} s ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage to load chemprop dataset: 290.1875 MB \n",
      "Time to load streaming dataset: 11.422368049621582 s \n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "start_time = time.time()\n",
    "memory_before = memory_record()\n",
    "dataset = dataset_preparator(\n",
    "    df=df,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column\n",
    ")\n",
    "memory_after = memory_record()\n",
    "end_time = time.time()\n",
    "gc.collect()\n",
    "\n",
    "print(f'Memory usage to load chemprop dataset: {memory_after-memory_before} MB ')\n",
    "print(f'Time to load streaming dataset: {end_time-start_time} s ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test 2: Similarity to chemprop data loader**\n",
    "\n",
    "In this test, I aim to demonstrate that the function works similarly to the Chemprop data loader. Additionally, we can apply a scaler if necessary; however, it is important to fit the scaler on the entire dataset (Pandas DataFrame) before applying it.\n",
    "\n",
    "For illustration purposes, I will only take 10 instances from the whole dataset for examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train with unscaled target values: \n",
      "876243   -7.48446\n",
      "607660   -7.04395\n",
      "214620   -7.32907\n",
      "864331   -8.18819\n",
      "669759   -7.69349\n",
      "559369   -5.37299\n",
      "39825    -5.69807\n",
      "681769   -7.51751\n",
      "285017   -7.58660\n",
      "732176   -6.32177\n",
      "Name: docking_score, dtype: float64\n",
      "----------------------------------------\n",
      "df_train with scaled target values: \n",
      "0   -0.841697\n",
      "1   -0.421607\n",
      "2   -0.693510\n",
      "3   -1.512807\n",
      "4   -1.041038\n",
      "5    1.171899\n",
      "6    0.861888\n",
      "7   -0.873215\n",
      "8   -0.939103\n",
      "9    0.267098\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xg/drggnx1n5cv7mnctplfkwlzh0000gn/T/ipykernel_88974/3754977134.py:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('on_the_fly_data.csv')\n"
     ]
    }
   ],
   "source": [
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "\n",
    "df_train = pd.read_csv('on_the_fly_data.csv')\n",
    "df_train_10 = df_train.sample(10)\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "\n",
    "print(f'df_train with unscaled target values: \\n{df_train_10.docking_score}')\n",
    "print('-'*40)\n",
    "print(f'df_train with scaled target values: \\n{pd.Series(scaler.transform(df_train_10[[target_column]]).reshape(-1))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chemprop Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with Chemprop data loader\n",
      "Batch 1\n",
      "tensor([[-7.4845],\n",
      "        [-7.0440],\n",
      "        [-7.3291],\n",
      "        [-8.1882],\n",
      "        [-7.6935]])\n",
      "Batch 2\n",
      "tensor([[-5.3730],\n",
      "        [-5.6981],\n",
      "        [-7.5175],\n",
      "        [-7.5866],\n",
      "        [-6.3218]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-7.4845],\n",
      "        [-7.0440],\n",
      "        [-7.3291],\n",
      "        [-8.1882],\n",
      "        [-7.6935]])\n",
      "Batch 2\n",
      "tensor([[-5.3730],\n",
      "        [-5.6981],\n",
      "        [-7.5175],\n",
      "        [-7.5866],\n",
      "        [-6.3218]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_preparator(df_train_10, smiles_column, target_column)\n",
    "train_loader = data.build_dataloader(train_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "print('Data batches with Chemprop data loader')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with StreamingMolDataset:\n",
      "Batch 1\n",
      "tensor([[-7.4845],\n",
      "        [-7.0440],\n",
      "        [-7.3291],\n",
      "        [-8.1882],\n",
      "        [-7.6935]])\n",
      "Batch 2\n",
      "tensor([[-5.3730],\n",
      "        [-5.6981],\n",
      "        [-7.5175],\n",
      "        [-7.5866],\n",
      "        [-6.3218]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-7.4845],\n",
      "        [-7.0440],\n",
      "        [-7.3291],\n",
      "        [-8.1882],\n",
      "        [-7.6935]])\n",
      "Batch 2\n",
      "tensor([[-5.3730],\n",
      "        [-5.6981],\n",
      "        [-7.5175],\n",
      "        [-7.5866],\n",
      "        [-6.3218]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_streaming_dataset = StreamingMolDataset(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    batch_size=5, scaler=None, shuffle=False\n",
    ")\n",
    "\n",
    "train_streaming_loader = DataLoader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=5,\n",
    "    collate_fn=collate_batch)\n",
    "\n",
    "print('Data batches with StreamingMolDataset:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_streaming_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point:** Without shuffling, the results indicated that the Chemprop dataset and the Streaming dataset behaved similarly. One advantage of the Streaming dataset is that it doesn't require generating all data points at once. \n",
    "\n",
    "Additionally, we can apply scaling to it, especially during training. However, this requires fitting an external scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Scaled target values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with scaled target values:\n",
      "Batch 1\n",
      "tensor([[-0.8417],\n",
      "        [-0.4216],\n",
      "        [-0.6935],\n",
      "        [-1.5128],\n",
      "        [-1.0410]])\n",
      "Batch 2\n",
      "tensor([[ 1.1719],\n",
      "        [ 0.8619],\n",
      "        [-0.8732],\n",
      "        [-0.9391],\n",
      "        [ 0.2671]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-0.8417],\n",
      "        [-0.4216],\n",
      "        [-0.6935],\n",
      "        [-1.5128],\n",
      "        [-1.0410]])\n",
      "Batch 2\n",
      "tensor([[ 1.1719],\n",
      "        [ 0.8619],\n",
      "        [-0.8732],\n",
      "        [-0.9391],\n",
      "        [ 0.2671]])\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/lod/miniconda3/envs/chemprop_2/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# DataLoader with batch_size = 5 (2 batches) and scaled\n",
    "scaler = StandardScaler().fit(df_train[[target_column]]) # Fit on the whole train_data.\n",
    "\n",
    "train_streaming_dataset = StreamingMolDataset(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    batch_size=5, scaler=scaler, shuffle=False\n",
    ")\n",
    "\n",
    "train_streaming_loader = DataLoader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=5,\n",
    "    collate_fn=collate_batch)\n",
    "\n",
    "print('Data batches with scaled target values:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_streaming_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingMolDataset Dataloader: batch_size = 5 (2 batches) and Unscaled target values:**\n",
    "\n",
    "In this part, when shuffle is activated, the samples in each batch are different between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batches with unscaled target values:\n",
      "Batch 1\n",
      "tensor([[-7.0440],\n",
      "        [-7.3291],\n",
      "        [-7.5175],\n",
      "        [-8.1882],\n",
      "        [-5.3730]])\n",
      "Batch 2\n",
      "tensor([[-7.4845],\n",
      "        [-6.3218],\n",
      "        [-5.6981],\n",
      "        [-7.5866],\n",
      "        [-7.6935]])\n",
      "----------------------------------------\n",
      "Batch 1\n",
      "tensor([[-7.3291],\n",
      "        [-7.4845],\n",
      "        [-6.3218],\n",
      "        [-8.1882],\n",
      "        [-7.5866]])\n",
      "Batch 2\n",
      "tensor([[-7.6935],\n",
      "        [-5.3730],\n",
      "        [-7.0440],\n",
      "        [-5.6981],\n",
      "        [-7.5175]])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_streaming_dataset = StreamingMolDataset(\n",
    "    df=df_train_10,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    batch_size=5, scaler=None, shuffle=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=5,\n",
    "    collate_fn=collate_batch)\n",
    "\n",
    "print('Data batches with unscaled target values:')\n",
    "for epoch in range(2):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        print(f'Batch {i+1}')\n",
    "        print(batch.Y)\n",
    "    print('-'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
