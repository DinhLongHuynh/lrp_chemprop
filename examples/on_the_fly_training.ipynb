{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, models, nn, featurizers\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import IterableDataset\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import IterableDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath('../lrp_chemprop/'))\n",
    "from Data_Preprocessor import Data_Preprocessor\n",
    "from IterableMolDatapoints import IterableMolDatapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare data using IterableMolDatapoints. Check usage here: \n",
    "\n",
    "https://github.com/DinhLongHuynh/lrp_chemprop/blob/main/lrp_chemprop/IterableMolDatapoints.py\n",
    "\n",
    "https://medium.com/@dinhlong240600/large-dataset-on-8gb-ram-let-iterabledataset-handle-442bb4764c7a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3860846/3902945969.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../DRD2_diverse_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "weight_column = 'weight_lowscores'\n",
    "split_column = 'split'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(data_path)\n",
    "df_train = df[df[split_column]=='train']\n",
    "df_val = df[df[split_column]=='val']\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "\n",
    "\n",
    "train_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_train,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=True, size_at_time=640)\n",
    "\n",
    "train_loader = data.build_dataloader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "val_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_val,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=False, size_at_time=640)\n",
    "\n",
    "val_loader = data.build_dataloader(\n",
    "    val_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our model. The parameters can be added by hands or using .toml, .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish model (Parameter can be modify manually or add from .tmol file)\n",
    "mp = nn.BondMessagePassing(d_v = 74, d_e = 14, d_h = 300,\n",
    "                           dropout=0.3,\n",
    "                           depth=5)\n",
    "\n",
    "agg = nn.NormAggregation(norm=199)\n",
    "\n",
    "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "\n",
    "ffn = nn.RegressionFFN(n_layers=2,\n",
    "                       dropout=0.3,\n",
    "                       input_dim=300,\n",
    "                       hidden_dim=2200,\n",
    "                       output_transform=output_transform)\n",
    "                       \n",
    "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "\n",
    "mpnn = models.MPNN(message_passing=mp, \n",
    "                   agg = agg, \n",
    "                   predictor=ffn, \n",
    "                   batch_norm=False, \n",
    "                   metrics=metric_list,\n",
    "                   warmup_epochs=1,\n",
    "                   init_lr=1.477783789959149e-06,\n",
    "                   max_lr=0.00012044152141486488,\n",
    "                   final_lr=0.00011724292252282861)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing = ModelCheckpoint(\n",
    "    \"../hyperparam_optim_7/model_7/checkpoints\",  # Directory where model checkpoints will be saved\n",
    "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=[checkpointing]\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
