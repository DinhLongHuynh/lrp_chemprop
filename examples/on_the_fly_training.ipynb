{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, models, nn\n",
    "import json\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from chemprop import data, featurizers\n",
    "from chemprop.featurizers.molecule import MorganBinaryFeaturizer\n",
    "import math\n",
    "import math\n",
    "from torch.utils.data import IterableDataset\n",
    "from chemprop.data.collate import collate_batch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preprocessor:\n",
    "    '''A class to prepare Chemprop dataset from Pandas dataframe.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def is_hbd(self,atom):\n",
    "        '''Check if an atom is a Hydrogen Bond Donor (HBD). An atom is considered an HBD if it's N or O with at least one hydrogen.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        atom: RDKit atom object.\n",
    "            \n",
    "        Returns:\n",
    "        ----------\n",
    "        bool: True if atom is HBD, False otherwise.\n",
    "        '''\n",
    "        \n",
    "        if atom.GetAtomicNum() not in [7, 8]:  # 7 for N, 8 for O\n",
    "            return False\n",
    "        \n",
    "        n_hydrogens = atom.GetTotalNumHs()\n",
    "        return n_hydrogens > 0\n",
    "\n",
    "    \n",
    "    def is_hba(self,atom):\n",
    "        '''Check if an atom is a Hydrogen Bond Acceptor (HBA). An atom is considered an HBA if it's N or O with a lone pair electron\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        atom: RDKit atom object.\n",
    "            \n",
    "        Returns:\n",
    "        ----------\n",
    "        bool: True if atom is HBD, False otherwise.\n",
    "        '''\n",
    "        \n",
    "        atomic_num = atom.GetAtomicNum()\n",
    "        if atomic_num not in [7, 8]: \n",
    "            return False\n",
    "        \n",
    "        valence = atom.GetTotalValence()\n",
    "        if atomic_num == 7:  \n",
    "            return valence <= 3 \n",
    "        else: \n",
    "            return valence <= 2  \n",
    "        \n",
    "\n",
    "    def get_mol_HBD_HBA(self,mols):\n",
    "        '''A function to generate HBD_HBA properties for molecules\n",
    "        \n",
    "        Parameters:\n",
    "        ---------\n",
    "        mols (list): list of RDKit mol objects.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        mol_HBs (list): list of array that contain HBD-HBA descriptor for molecules, shape of each array is (n_atom, 2)  '''\n",
    "        mol_HBs = []\n",
    "        for mol in mols:\n",
    "            mol_HB = [[],[]]\n",
    "            for atom in mol.GetAtoms():\n",
    "                if self.is_hbd(atom):\n",
    "                    mol_HB[0].append(1)\n",
    "                else:\n",
    "                    mol_HB[0].append(0)\n",
    "                    \n",
    "                if self.is_hba(atom):\n",
    "                    mol_HB[1].append(1)\n",
    "                else:\n",
    "                    mol_HB[1].append(0)\n",
    "            mol_HB = np.array(mol_HB).T\n",
    "            mol_HBs.append(mol_HB)\n",
    "        return mol_HBs\n",
    "\n",
    "    \n",
    "\n",
    "    def dataset_generator(self):\n",
    "        '''Prepare chemprop dataset without additional HBD/HBA feature.\n",
    "    \n",
    "        Returns:\n",
    "        ----------\n",
    "        dataset (Chemprop dataset): Chemprop dataset.\n",
    "        '''\n",
    "        \n",
    "        morgan_fp = MorganBinaryFeaturizer()\n",
    "        def datapoint_generator(df,smiles,y,addH,HB,morgan,weight):\n",
    "            smis = df.loc[:,smiles].values\n",
    "            ys = df.loc[:,[y]].values\n",
    "            mols = [Chem.MolFromSmiles(smi) for smi in smis]\n",
    "\n",
    "            if weight!= None:\n",
    "                weights = df.loc[:,weight].values\n",
    "\n",
    "            if HB:\n",
    "                mol_HBs = self.get_mol_HBD_HBA(mols)\n",
    "            else:\n",
    "                mol_HBs = [None]*len(smis)\n",
    "\n",
    "            if morgan:\n",
    "                x_ds = [morgan_fp(mol) for mol in mols]\n",
    "            else:\n",
    "                x_ds = [None]*len(smis)\n",
    "            \n",
    "            datapoints = [data.MoleculeDatapoint.from_smi(smi,y,add_h=addH, V_f = mol_HB, x_d = x_d, weight=weight) for smi, y, mol_HB, x_d, weight in zip(smis,ys,mol_HBs,x_ds, weights)]\n",
    "            return datapoints\n",
    "\n",
    "        datapoints = datapoint_generator(df=self.df,smiles=self.smiles_column,y=self.target_column,addH=self.addH,HB=self.HB,morgan=self.morgan,weight=self.weight_column)\n",
    "        dataset = data.MoleculeDataset(datapoints, featurizer=self.featurizer)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def generate(self, df, smiles_column = 'smiles', target_column='docking_score', addH=False, HB = False, morgan = False, weight_column =None,\n",
    "                 featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()):\n",
    "        '''Generate chemprop dataset according to a given configuration\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df (Pandas DataFrame): a data frame that contains SMILES code of compounds.\n",
    "        smiles_column (str): a string that indicates SMILES column in the data frame.\n",
    "        target_column (str): a string that indicates the target column (i.e. docking_scores, solubility) in the data frame.\n",
    "        addH (boolean): to incorporate explicit hydrogen atoms into a molecular graph.\n",
    "        HB (boolean): to incorporate additional HBD/HBA features for each atom in BatchMolGraph.\n",
    "        morgan (boolean): to incorporate morgan binaray fingerprint for each molecules\n",
    "        featurizer (Chemprop Featurizer): a Featurizer from Chemprop to encode features for atoms, bonds, and molecules.\n",
    "    \n",
    "        Returns:\n",
    "        ----------\n",
    "        dataset (Chemprop dataset): Chemprop dataset\n",
    "        '''\n",
    "                     \n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.addH = addH\n",
    "        self.HB = HB\n",
    "        self.featurizer = featurizer\n",
    "        self.morgan = morgan\n",
    "        self.weight_column = weight_column\n",
    "        \n",
    "\n",
    "        return self.dataset_generator()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapoint_preparator(df,smiles_column,target_column,weight_column):\n",
    "    smis = df.loc[:,smiles_column].values\n",
    "    ys = df.loc[:,[target_column]].values\n",
    "    weights = df.loc[:,weight_column].values\n",
    "            \n",
    "    datapoints = [data.MoleculeDatapoint.from_smi(smi,y,weight=weight) for smi, y, weight in zip(smis,ys,weights)]\n",
    "    return datapoints\n",
    "\n",
    "\n",
    "def dataset_preparator(df, smiles_column, target_column, weight_column, featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()):\n",
    "    datapoints = datapoint_preparator(df=df, smiles_column=smiles_column, target_column=target_column,weight_column=weight_column)\n",
    "    dataset = data.MoleculeDataset(datapoints, featurizer=featurizer)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMolDatapoints(IterableDataset):\n",
    "    '''A class to prepare data for streaming, which is a subclass of IterableDataset. \n",
    "    The output is a generator that yields one chemprop.data.datasets.Datum at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df, smiles_column, target_column, weight_column, scaler = None, size_at_time=100, shuffle=True):\n",
    "        '''Parameters:\n",
    "        ----------\n",
    "        df (pd.DataFrame): A pandas dataframe containing the data.\n",
    "        smiles_column (str): The column name containing SMILES strings.\n",
    "        target_column (str): The column name containing the target values.\n",
    "        scaler (StandardScaler): A StandardScaler object (already fitted) for normalizing the target values.\n",
    "        size_at_time (int): The number of samples to transfrom into chemprop.data.datasets.Datum at a time.\n",
    "        shuffle (boolean): If the df is shuffled.'''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.weight_column = weight_column\n",
    "        self.size_at_time = size_at_time\n",
    "        self.shuffle= shuffle\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''A function to define iteration logic. It take the whole csv data, then shuffled, then access to only a subset of data at a time for transformation.\n",
    "        The output is a generator that yields chemprop.data.datasets.Datum and ready to put through DataLoader.\n",
    "        '''\n",
    "\n",
    "        if self.shuffle:\n",
    "            df_shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            df_shuffled = self.df.copy()\n",
    "\n",
    "        # Transform pandas dataframe to molecule dataset according to size_at_time, prevent overloading memory. This is to balance between memory and speed.\n",
    "        for i in range(0, len(df_shuffled), self.size_at_time):\n",
    "            df_at_time = df_shuffled.iloc[i:i + self.size_at_time]\n",
    "            data_generator = Data_Preprocessor()\n",
    "            df_process = data_generator.generate(df=df_at_time,smiles_column=self.smiles_column,target_column=self.target_column,HB=True,weight_column='weight_lowscores')\n",
    "\n",
    "            if self.scaler != None: \n",
    "                df_process.normalize_targets(scaler = self.scaler)\n",
    "\n",
    "            # Handling parallelization manually\n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            if worker_info is None: \n",
    "                for mol in df_process:\n",
    "                    yield mol\n",
    "            else: \n",
    "                num_workers = worker_info.num_workers\n",
    "                worker_id = worker_info.id\n",
    "                for i, mol in enumerate(df_process):\n",
    "                    if i % num_workers == worker_id:\n",
    "                        yield mol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3860846/3902945969.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../DRD2_diverse_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "weight_column = 'weight_lowscores'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(data_path)\n",
    "df_train = df[df['split_random_1']!='test']\n",
    "df_val = df[df['split_random_1']=='test']\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "\n",
    "\n",
    "train_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_train,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=True, size_at_time=640)\n",
    "\n",
    "train_loader = data.build_dataloader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "val_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_val,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=False, size_at_time=640)\n",
    "\n",
    "val_loader = data.build_dataloader(\n",
    "    val_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish model\n",
    "mp = nn.BondMessagePassing(d_v = 74, d_e = 14, d_h = 300,\n",
    "                           dropout=0.3,\n",
    "                           depth=5)\n",
    "\n",
    "agg = nn.NormAggregation(norm=199)\n",
    "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "ffn = nn.RegressionFFN(n_layers=2,\n",
    "                       dropout=0.3,\n",
    "                       input_dim=300,\n",
    "                       hidden_dim=2200,\n",
    "                       output_transform=output_transform)\n",
    "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "\n",
    "mpnn = models.MPNN(message_passing=mp, \n",
    "                   agg = agg, \n",
    "                   predictor=ffn, \n",
    "                   batch_norm=False, \n",
    "                   metrics=metric_list,\n",
    "                   warmup_epochs=1,\n",
    "                   init_lr=1.477783789959149e-06,\n",
    "                   max_lr=0.00012044152141486488,\n",
    "                   final_lr=0.00011724292252282861)\n",
    "\n",
    "#mpnn = models.MPNN.load_from_checkpoint('../../../hyperparam_optim_5/best_checkpoint.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:122: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 228 K  | train\n",
      "1 | agg             | NormAggregation    | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 5.5 M  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "5.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 M     Total params\n",
      "22.942    Total estimated model params size (MB)\n",
      "28        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fc1a305a9d485792a695ee7ec4d1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436367ad6fc5427b818c25a7ed4f83f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:269\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m--> 269\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n\u001b[1;32m    270\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             fn(trainer, trainer\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/tqdm_progress.py:278\u001b[0m, in \u001b[0;36mTQDMProgressBar.on_train_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_update(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mtotal):\n\u001b[0;32m--> 278\u001b[0m     _update_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar, n)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics(trainer, pl_module))\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/tqdm_progress.py:459\u001b[0m, in \u001b[0;36m_update_n\u001b[0;34m(bar, value)\u001b[0m\n\u001b[1;32m    458\u001b[0m bar\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 459\u001b[0m bar\u001b[38;5;241m.\u001b[39mrefresh()\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/tqdm/notebook.py:157\u001b[0m, in \u001b[0;36mtqdm_notebook.display\u001b[0;34m(self, msg, pos, close, bar_style, check_delay)\u001b[0m\n\u001b[1;32m    156\u001b[0m ltext, pbar, rtext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mchildren\n\u001b[0;32m--> 157\u001b[0m pbar\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msg:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/traitlets/traitlets.py:716\u001b[0m, in \u001b[0;36mTraitType.__set__\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraitError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m trait is read-only.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset(obj, value)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/traitlets/traitlets.py:706\u001b[0m, in \u001b[0;36mTraitType.set\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# we explicitly compare silent to True just in case the equality\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# comparison above returns something other than True/False\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_notify_trait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, old_value, new_value)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/traitlets/traitlets.py:1513\u001b[0m, in \u001b[0;36mHasTraits._notify_trait\u001b[0;34m(self, name, old_value, new_value)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_notify_trait\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, old_value: t\u001b[38;5;241m.\u001b[39mAny, new_value: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_change(\n\u001b[1;32m   1514\u001b[0m         Bunch(\n\u001b[1;32m   1515\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1516\u001b[0m             old\u001b[38;5;241m=\u001b[39mold_value,\n\u001b[1;32m   1517\u001b[0m             new\u001b[38;5;241m=\u001b[39mnew_value,\n\u001b[1;32m   1518\u001b[0m             owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1519\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1520\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipywidgets/widgets/widget.py:700\u001b[0m, in \u001b[0;36mWidget.notify_change\u001b[0;34m(self, change)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_send_property(name, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)):\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# Send new state to front-end\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_state(key\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mnotify_change(change)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipywidgets/widgets/widget.py:586\u001b[0m, in \u001b[0;36mWidget.send_state\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    585\u001b[0m msg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer_paths\u001b[39m\u001b[38;5;124m'\u001b[39m: buffer_paths}\n\u001b[0;32m--> 586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(msg, buffers\u001b[38;5;241m=\u001b[39mbuffers)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipywidgets/widgets/widget.py:825\u001b[0m, in \u001b[0;36mWidget._send\u001b[0;34m(self, msg, buffers)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomm\u001b[38;5;241m.\u001b[39msend(data\u001b[38;5;241m=\u001b[39mmsg, buffers\u001b[38;5;241m=\u001b[39mbuffers)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/comm/base_comm.py:147\u001b[0m, in \u001b[0;36mBaseComm.send\u001b[0;34m(self, data, metadata, buffers)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a message to the frontend-side version of this comm\"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpublish_msg(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomm_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    149\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    150\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    151\u001b[0m     buffers\u001b[38;5;241m=\u001b[39mbuffers,\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipykernel/comm/comm.py:37\u001b[0m, in \u001b[0;36mBaseComm.publish_msg\u001b[0;34m(self, msg_type, data, metadata, buffers, **keys)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39miopub_socket,\n\u001b[1;32m     39\u001b[0m     msg_type,\n\u001b[1;32m     40\u001b[0m     content,\n\u001b[1;32m     41\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mjson_clean(metadata),\n\u001b[1;32m     42\u001b[0m     parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mget_parent(),\n\u001b[1;32m     43\u001b[0m     ident\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic,\n\u001b[1;32m     44\u001b[0m     buffers\u001b[38;5;241m=\u001b[39mbuffers,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/jupyter_client/session.py:863\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[1;32m    862\u001b[0m     tracker \u001b[38;5;241m=\u001b[39m DONE\n\u001b[0;32m--> 863\u001b[0m     stream\u001b[38;5;241m.\u001b[39msend_multipart(to_send, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipykernel/iostream.py:346\u001b[0m, in \u001b[0;36mBackgroundSocket.send_multipart\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio_thread\u001b[38;5;241m.\u001b[39msend_multipart(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipykernel/iostream.py:276\u001b[0m, in \u001b[0;36mIOPubThread.send_multipart\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"send_multipart schedules actual zmq send in my thread.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mIf my thread isn't running (e.g. forked process), send immediately.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_really_send(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_pipe\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/zmq/sugar/socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    695\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags\u001b[38;5;241m=\u001b[39mflags, copy\u001b[38;5;241m=\u001b[39mcopy, track\u001b[38;5;241m=\u001b[39mtrack)\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/zmq/backend/cython/checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 24\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpointing \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../hyperparam_optim_7/model_7/checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Directory where model checkpoints will be saved\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest-\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_loss:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Filename format for checkpoints, including epoch and validation loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     save_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Always save the most recent checkpoint, even if it's not the best\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpointing]\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(mpnn, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader, val_dataloaders\u001b[38;5;241m=\u001b[39mval_loader)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    540\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
    "\n",
    "\n",
    "checkpointing = ModelCheckpoint(\n",
    "    \"../hyperparam_optim_7/model_7/checkpoints\",  # Directory where model checkpoints will be saved\n",
    "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=[checkpointing]\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50\n",
    ")\n",
    "\n",
    "val_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_val,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=None, shuffle=False, size_at_time=640)\n",
    "\n",
    "val_loader = data.build_dataloader(\n",
    "    val_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/chemprop/models/model.py:286: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(checkpoint_path, map_location)\n"
     ]
    }
   ],
   "source": [
    "model = models.MPNN.load_from_checkpoint('../hyperparam_optim_5/best_checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:122: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c01cfd221d44574a66bfa0025bec8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.45445483922958374    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7185391783714294     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/rmse         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.579860270023346     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.45445483922958374   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7185391783714294    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/rmse        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.579860270023346    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/rmse': 0.579860270023346,\n",
       "  'test/mae': 0.45445483922958374,\n",
       "  'test/r2': 0.7185391783714294}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
