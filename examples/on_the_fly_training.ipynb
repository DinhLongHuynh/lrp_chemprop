{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, models, nn, featurizers\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import IterableDataset\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import IterableDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.abspath('../lrp_chemprop/'))\n",
    "from Data_Preprocessor import Data_Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMolDatapoints(IterableDataset):\n",
    "    '''A class to prepare data for streaming, which is a subclass of IterableDataset. \n",
    "    The output is a generator that yields one chemprop.data.datasets.Datum at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df, smiles_column, target_column, weight_column, scaler = None, size_at_time=100, shuffle=True):\n",
    "        '''Parameters:\n",
    "        ----------\n",
    "        df (pd.DataFrame): A pandas dataframe containing the data.\n",
    "        smiles_column (str): The column name containing SMILES strings.\n",
    "        target_column (str): The column name containing the target values.\n",
    "        scaler (StandardScaler): A StandardScaler object (already fitted) for normalizing the target values.\n",
    "        size_at_time (int): The number of samples to transfrom into chemprop.data.datasets.Datum at a time.\n",
    "        shuffle (boolean): If the df is shuffled.'''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.weight_column = weight_column\n",
    "        self.size_at_time = size_at_time\n",
    "        self.shuffle= shuffle\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''A function to define iteration logic. It take the whole csv data, then shuffled, then access to only a subset of data at a time for transformation.\n",
    "        The output is a generator that yields chemprop.data.datasets.Datum and ready to put through DataLoader.\n",
    "        '''\n",
    "\n",
    "        if self.shuffle:\n",
    "            df_shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            df_shuffled = self.df.copy()\n",
    "\n",
    "        # Transform pandas dataframe to molecule dataset according to size_at_time, prevent overloading memory. This is to balance between memory and speed.\n",
    "        for i in range(0, len(df_shuffled), self.size_at_time):\n",
    "            df_at_time = df_shuffled.iloc[i:i + self.size_at_time]\n",
    "            data_generator = Data_Preprocessor()\n",
    "            df_process = data_generator.generate(df=df_at_time,smiles_column=self.smiles_column,target_column=self.target_column,HB=True,weight_column='weight_lowscores')\n",
    "\n",
    "            if self.scaler != None: \n",
    "                df_process.normalize_targets(scaler = self.scaler)\n",
    "\n",
    "            # Handling parallelization manually\n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            if worker_info is None: \n",
    "                for mol in df_process:\n",
    "                    yield mol\n",
    "            else: \n",
    "                num_workers = worker_info.num_workers\n",
    "                worker_id = worker_info.id\n",
    "                for i, mol in enumerate(df_process):\n",
    "                    if i % num_workers == worker_id:\n",
    "                        yield mol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3860846/3902945969.py:9: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../DRD2_diverse_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "weight_column = 'weight_lowscores'\n",
    "split_column = 'split'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(data_path)\n",
    "df_train = df[df[split_column]=='train']\n",
    "df_val = df[df[split_column]=='val']\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "\n",
    "\n",
    "train_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_train,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=True, size_at_time=640)\n",
    "\n",
    "train_loader = data.build_dataloader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "val_streaming_dataset = IterableMolDatapoints(\n",
    "    df=df_val,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    weight_column=weight_column,\n",
    "    scaler=scaler, shuffle=False, size_at_time=640)\n",
    "\n",
    "val_loader = data.build_dataloader(\n",
    "    val_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish model (Parameter can be modify manually or add from .tmol file)\n",
    "mp = nn.BondMessagePassing(d_v = 74, d_e = 14, d_h = 300,\n",
    "                           dropout=0.3,\n",
    "                           depth=5)\n",
    "\n",
    "agg = nn.NormAggregation(norm=199)\n",
    "output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
    "ffn = nn.RegressionFFN(n_layers=2,\n",
    "                       dropout=0.3,\n",
    "                       input_dim=300,\n",
    "                       hidden_dim=2200,\n",
    "                       output_transform=output_transform)\n",
    "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "\n",
    "mpnn = models.MPNN(message_passing=mp, \n",
    "                   agg = agg, \n",
    "                   predictor=ffn, \n",
    "                   batch_norm=False, \n",
    "                   metrics=metric_list,\n",
    "                   warmup_epochs=1,\n",
    "                   init_lr=1.477783789959149e-06,\n",
    "                   max_lr=0.00012044152141486488,\n",
    "                   final_lr=0.00011724292252282861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointing = ModelCheckpoint(\n",
    "    \"../hyperparam_optim_7/model_7/checkpoints\",  # Directory where model checkpoints will be saved\n",
    "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=[checkpointing]\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
