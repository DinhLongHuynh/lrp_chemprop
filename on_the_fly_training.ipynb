{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from lightning import pytorch as pl\n",
    "from chemprop import data, models, nn\n",
    "import json\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import IterableDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from chemprop import data\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from chemprop import data, featurizers\n",
    "from chemprop.featurizers.molecule import MorganBinaryFeaturizer\n",
    "import math\n",
    "import math\n",
    "from torch.utils.data import IterableDataset\n",
    "from chemprop.data.collate import collate_batch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preprocessor:\n",
    "    '''A class to prepare Chemprop dataset from Pandas dataframe.'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def is_hbd(self,atom):\n",
    "        '''Check if an atom is a Hydrogen Bond Donor (HBD). An atom is considered an HBD if it's N or O with at least one hydrogen.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        atom: RDKit atom object.\n",
    "            \n",
    "        Returns:\n",
    "        ----------\n",
    "        bool: True if atom is HBD, False otherwise.\n",
    "        '''\n",
    "        \n",
    "        if atom.GetAtomicNum() not in [7, 8]:  # 7 for N, 8 for O\n",
    "            return False\n",
    "        \n",
    "        n_hydrogens = atom.GetTotalNumHs()\n",
    "        return n_hydrogens > 0\n",
    "\n",
    "    \n",
    "    def is_hba(self,atom):\n",
    "        '''Check if an atom is a Hydrogen Bond Acceptor (HBA). An atom is considered an HBA if it's N or O with a lone pair electron\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        atom: RDKit atom object.\n",
    "            \n",
    "        Returns:\n",
    "        ----------\n",
    "        bool: True if atom is HBD, False otherwise.\n",
    "        '''\n",
    "        \n",
    "        atomic_num = atom.GetAtomicNum()\n",
    "        if atomic_num not in [7, 8]: \n",
    "            return False\n",
    "        \n",
    "        valence = atom.GetTotalValence()\n",
    "        if atomic_num == 7:  \n",
    "            return valence <= 3 \n",
    "        else: \n",
    "            return valence <= 2  \n",
    "        \n",
    "\n",
    "    def get_mol_HBD_HBA(self,mols):\n",
    "        '''A function to generate HBD_HBA properties for molecules\n",
    "        \n",
    "        Parameters:\n",
    "        ---------\n",
    "        mols (list): list of RDKit mol objects.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        mol_HBs (list): list of array that contain HBD-HBA descriptor for molecules, shape of each array is (n_atom, 2)  '''\n",
    "        mol_HBs = []\n",
    "        for mol in mols:\n",
    "            mol_HB = [[],[]]\n",
    "            for atom in mol.GetAtoms():\n",
    "                if self.is_hbd(atom):\n",
    "                    mol_HB[0].append(1)\n",
    "                else:\n",
    "                    mol_HB[0].append(0)\n",
    "                    \n",
    "                if self.is_hba(atom):\n",
    "                    mol_HB[1].append(1)\n",
    "                else:\n",
    "                    mol_HB[1].append(0)\n",
    "            mol_HB = np.array(mol_HB).T\n",
    "            mol_HBs.append(mol_HB)\n",
    "        return mol_HBs\n",
    "\n",
    "    \n",
    "\n",
    "    def dataset_generator(self):\n",
    "        '''Prepare chemprop dataset without additional HBD/HBA feature.\n",
    "    \n",
    "        Returns:\n",
    "        ----------\n",
    "        dataset (Chemprop dataset): Chemprop dataset.\n",
    "        '''\n",
    "        \n",
    "        morgan_fp = MorganBinaryFeaturizer()\n",
    "        def datapoint_generator(df,smiles,y,addH,HB,morgan):\n",
    "            smis = df.loc[:,smiles].values\n",
    "            ys = df.loc[:,[y]].values\n",
    "            mols = [Chem.MolFromSmiles(smi) for smi in smis]\n",
    "\n",
    "            if HB:\n",
    "                mol_HBs = self.get_mol_HBD_HBA(mols)\n",
    "            else:\n",
    "                mol_HBs = [None]*len(smis)\n",
    "\n",
    "            if morgan:\n",
    "                x_ds = [morgan_fp(mol) for mol in mols]\n",
    "            else:\n",
    "                x_ds = [None]*len(smis)\n",
    "            \n",
    "            datapoints = [data.MoleculeDatapoint.from_smi(smi,y,add_h=addH, V_f = mol_HB, x_d = x_d) for smi, y, mol_HB, x_d in zip(smis,ys,mol_HBs,x_ds)]\n",
    "            return datapoints\n",
    "\n",
    "        datapoints = datapoint_generator(df=self.df,smiles=self.smiles_column,y=self.target_column,addH=self.addH,HB=self.HB,morgan=self.morgan)\n",
    "        dataset = data.MoleculeDataset(datapoints, featurizer=self.featurizer)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def generate(self, df, smiles_column = 'smiles', target_column='docking_score', addH=False, HB = False, morgan = False,\n",
    "                 featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()):\n",
    "        '''Generate chemprop dataset according to a given configuration\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df (Pandas DataFrame): a data frame that contains SMILES code of compounds.\n",
    "        smiles_column (str): a string that indicates SMILES column in the data frame.\n",
    "        target_column (str): a string that indicates the target column (i.e. docking_scores, solubility) in the data frame.\n",
    "        addH (boolean): to incorporate explicit hydrogen atoms into a molecular graph.\n",
    "        HB (boolean): to incorporate additional HBD/HBA features for each atom in BatchMolGraph.\n",
    "        morgan (boolean): to incorporate morgan binaray fingerprint for each molecules\n",
    "        featurizer (Chemprop Featurizer): a Featurizer from Chemprop to encode features for atoms, bonds, and molecules.\n",
    "    \n",
    "        Returns:\n",
    "        ----------\n",
    "        dataset (Chemprop dataset): Chemprop dataset\n",
    "        '''\n",
    "                     \n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.addH = addH\n",
    "        self.HB = HB\n",
    "        self.featurizer = featurizer\n",
    "        self.morgan = morgan\n",
    "        \n",
    "\n",
    "        return self.dataset_generator()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingMolDataset(IterableDataset):\n",
    "    def __init__(self, df, smiles_column, target_column, data_generator, scaler, batch_size=64, shuffle=True):\n",
    "        self.df = df\n",
    "        self.smiles_column = smiles_column\n",
    "        self.target_column = target_column\n",
    "        self.data_generator = data_generator\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle= shuffle\n",
    "        self.scaler = scaler\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle self.df at the start of each epoch\n",
    "        if self.shuffle:\n",
    "            df_shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            df_shuffled = self.df.copy()\n",
    "\n",
    "        # Process data in batches, yielding each batch of processed data\n",
    "        for i in range(0, len(self.df), self.batch_size):\n",
    "            df_chunk = self.df.iloc[i:i + self.batch_size]\n",
    "            # Generate processed data using the data generator\n",
    "            df_process = self.data_generator.generate(\n",
    "                df=df_chunk,\n",
    "                smiles_column=self.smiles_column,\n",
    "                target_column=self.target_column, addH =False, HB = False, morgan = False\n",
    "            )\n",
    "            df_process.normalize_targets(self.scaler)\n",
    "        \n",
    "        # Yield all the samples in the current batch\n",
    "            for mol in df_process:  # Debug what is being yielded\n",
    "                yield mol\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_135016/3100795657.py:13: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../DRD2_diverse_data.csv'\n",
    "smiles_column = 'smiles'\n",
    "target_column = 'docking_score'\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "model_config = '../../../hyperparam_optim_5/model.json'\n",
    "\n",
    "with open(model_config, 'r') as file:\n",
    "    model_config = json.load(file)\n",
    "    model_train_config = model_config['train_loop_config']\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(data_path)\n",
    "df_train = df[df['split_random_1']!='test']\n",
    "df_test = df[df['split_random_1']=='test']\n",
    "num_compounds = df_train.shape[0]\n",
    "\n",
    "# Establish model\n",
    "mp = nn.BondMessagePassing(d_h = model_train_config['message_hidden_dim'],\n",
    "                           dropout=model_train_config['dropout'],\n",
    "                           depth=model_train_config['depth'])\n",
    "\n",
    "agg = nn.SumAggregation()\n",
    "\n",
    "ffn = nn.RegressionFFN(n_layers=model_train_config['ffn_num_layers'],\n",
    "                       dropout=model_train_config['dropout'],\n",
    "                       input_dim=model_train_config['message_hidden_dim'],\n",
    "                       hidden_dim=model_train_config['ffn_hidden_dim'])\n",
    "metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()]\n",
    "\n",
    "mpnn = models.MPNN(message_passing=mp, \n",
    "                   agg = agg, \n",
    "                   predictor=ffn, \n",
    "                   batch_norm=False, \n",
    "                   metrics=metric_list,\n",
    "                   warmup_epochs=model_train_config['warmup_epochs'],\n",
    "                   init_lr=model_train_config['init_lr_ratio'],\n",
    "                   max_lr=model_train_config['max_lr'],\n",
    "                   final_lr=model_train_config['final_lr_ratio'])\n",
    "\n",
    "#mpnn = models.MPNN.load_from_checkpoint('../../../hyperparam_optim_5/best_checkpoint.ckpt')\n",
    "scaler = StandardScaler().fit(df_train[[target_column]])\n",
    "data_generator = Data_Preprocessor()\n",
    "\n",
    "\n",
    "train_streaming_dataset = StreamingMolDataset(\n",
    "    df=df_train,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    data_generator=data_generator, \n",
    "    batch_size=batch_size, scaler=scaler\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_batch)\n",
    "\n",
    "test_streaming_dataset = StreamingMolDataset(\n",
    "    df=df_test,\n",
    "    smiles_column=smiles_column,\n",
    "    target_column=target_column,\n",
    "    data_generator=data_generator,\n",
    "    batch_size=batch_size, scaler=scaler\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_streaming_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 11.9 M | train\n",
      "1 | agg             | SumAggregation     | 0      | train\n",
      "2 | bn              | Identity           | 0      | train\n",
      "3 | predictor       | RegressionFFN      | 5.5 M  | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "5 | metrics         | ModuleList         | 0      | train\n",
      "---------------------------------------------------------------\n",
      "17.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.4 M    Total params\n",
      "69.705    Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b987e4f8724722aa4bec4118c1d814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09d9a96a29b483c81f4337ae30c0332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:269\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m--> 269\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n\u001b[1;32m    270\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             fn(trainer, trainer\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/tqdm_progress.py:279\u001b[0m, in \u001b[0;36mTQDMProgressBar.on_train_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[0m\n\u001b[1;32m    278\u001b[0m _update_n(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar, n)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics(trainer, pl_module))\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/progress/progress_bar.py:198\u001b[0m, in \u001b[0;36mProgressBar.get_metrics\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    197\u001b[0m standard_metrics \u001b[38;5;241m=\u001b[39m get_standard_metrics(trainer)\n\u001b[0;32m--> 198\u001b[0m pbar_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mprogress_bar_metrics\n\u001b[1;32m    199\u001b[0m duplicates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(standard_metrics\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m&\u001b[39m pbar_metrics\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1635\u001b[0m, in \u001b[0;36mTrainer.progress_bar_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The metrics sent to the progress bar.\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03mThis includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m:paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \n\u001b[1;32m   1634\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mprogress_bar_metrics\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:253\u001b[0m, in \u001b[0;36m_LoggerConnector.progress_bar_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_results:\n\u001b[0;32m--> 253\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_bar_metrics\u001b[38;5;241m.\u001b[39mupdate(metrics)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:234\u001b[0m, in \u001b[0;36m_LoggerConnector.metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_results\u001b[38;5;241m.\u001b[39mmetrics(on_step)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:490\u001b[0m, in \u001b[0;36m_ResultCollection.metrics\u001b[0;34m(self, on_step)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result_metric\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprog_bar:\n\u001b[0;32m--> 490\u001b[0m         metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpbar\u001b[39m\u001b[38;5;124m\"\u001b[39m][forked_name] \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(value)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:136\u001b[0m, in \u001b[0;36mconvert_tensors_to_scalars\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_to_collection(data, Tensor, to_item)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py:64\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/fabric/utilities/apply_func.py:134\u001b[0m, in \u001b[0;36mconvert_tensors_to_scalars.<locals>.to_item\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe metric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` does not contain a single element, thus it cannot be converted to a scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 24\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpointing \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Directory where model checkpoints will be saved\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest-\u001b[39m\u001b[38;5;132;01m{epoch}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{val_loss:.2f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Filename format for checkpoints, including epoch and validation loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     save_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Always save the most recent checkpoint, even if it's not the best\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpointing]\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(mpnn, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader, val_dataloaders\u001b[38;5;241m=\u001b[39mtest_loader)\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    540\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/long_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
    "\n",
    "\n",
    "checkpointing = ModelCheckpoint(\n",
    "    \"checkpoints\",  # Directory where model checkpoints will be saved\n",
    "    \"best-{epoch}-{val_loss:.2f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
    "    \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
    "    mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
    "    save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=epochs,\n",
    "    callbacks=[checkpointing]\n",
    ")\n",
    "\n",
    "trainer.fit(mpnn, train_dataloaders=train_loader, val_dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>docking_score</th>\n",
       "      <th>split_random_1</th>\n",
       "      <th>split_random_2</th>\n",
       "      <th>split_random_3</th>\n",
       "      <th>split_random_4</th>\n",
       "      <th>split_random_5</th>\n",
       "      <th>weight_lowscores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15945052</td>\n",
       "      <td>CC(=O)OCC1(CC23CCC4C(C2CCC1C3)(CCCC4(C)C(=O)O)...</td>\n",
       "      <td>-0.023991</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42628272</td>\n",
       "      <td>CCOC(=O)C1C(=CCC(N1S(=O)(=O)C2=CC=C(C=C2)C)C3=...</td>\n",
       "      <td>-0.024816</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16187407</td>\n",
       "      <td>CCN(CC)CCC(C)OC(=O)C1=C(C=C(C=C1)O)O.Cl</td>\n",
       "      <td>-0.031822</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3580396</td>\n",
       "      <td>CC1CC2=CC=CC=C2N1C(=O)C3=CC=CC=C3NS(=O)(=O)C4=...</td>\n",
       "      <td>-0.043640</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9614845</td>\n",
       "      <td>CN(C)CCNC(=O)C=NO</td>\n",
       "      <td>-0.076320</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995754</th>\n",
       "      <td>CP002847469043</td>\n",
       "      <td>CCCCCN(CCCCC)CCOCCOCC</td>\n",
       "      <td>0.192226</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995755</th>\n",
       "      <td>CP003439278953</td>\n",
       "      <td>CC(C)(CN(C)C)C(=O)N(C)CCN(C)C(=O)C1(CCC1)N(C)C</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995756</th>\n",
       "      <td>CP002164348858</td>\n",
       "      <td>Cc1oc(c(c1)C(=O)N[C@H]2C[C@]3(C2)N(CCCC3)C(=O)...</td>\n",
       "      <td>0.596025</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995757</th>\n",
       "      <td>CP003228611624</td>\n",
       "      <td>CCC(CC)(CO)C(=O)N1CCN(C2(C1)CCC2)C(=O)C(=C(C)C)C</td>\n",
       "      <td>2.564700</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995758</th>\n",
       "      <td>CP002807128239</td>\n",
       "      <td>CC1(COC1)C(=O)N2CCC(CC2)C3C4(CCC4)CN3C(=O)CCSC</td>\n",
       "      <td>3.122500</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>val</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>995759 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                             smiles  \\\n",
       "0             15945052  CC(=O)OCC1(CC23CCC4C(C2CCC1C3)(CCCC4(C)C(=O)O)...   \n",
       "1             42628272  CCOC(=O)C1C(=CCC(N1S(=O)(=O)C2=CC=C(C=C2)C)C3=...   \n",
       "2             16187407            CCN(CC)CCC(C)OC(=O)C1=C(C=C(C=C1)O)O.Cl   \n",
       "3              3580396  CC1CC2=CC=CC=C2N1C(=O)C3=CC=CC=C3NS(=O)(=O)C4=...   \n",
       "4              9614845                                  CN(C)CCNC(=O)C=NO   \n",
       "...                ...                                                ...   \n",
       "995754  CP002847469043                              CCCCCN(CCCCC)CCOCCOCC   \n",
       "995755  CP003439278953     CC(C)(CN(C)C)C(=O)N(C)CCN(C)C(=O)C1(CCC1)N(C)C   \n",
       "995756  CP002164348858  Cc1oc(c(c1)C(=O)N[C@H]2C[C@]3(C2)N(CCCC3)C(=O)...   \n",
       "995757  CP003228611624   CCC(CC)(CO)C(=O)N1CCN(C2(C1)CCC2)C(=O)C(=C(C)C)C   \n",
       "995758  CP002807128239     CC1(COC1)C(=O)N2CCC(CC2)C3C4(CCC4)CN3C(=O)CCSC   \n",
       "\n",
       "        docking_score split_random_1 split_random_2 split_random_3  \\\n",
       "0           -0.023991            val          train          train   \n",
       "1           -0.024816          train          train          train   \n",
       "2           -0.031822          train          train            val   \n",
       "3           -0.043640          train          train          train   \n",
       "4           -0.076320          train          train          train   \n",
       "...               ...            ...            ...            ...   \n",
       "995754       0.192226          train          train            val   \n",
       "995755       0.436100          train          train          train   \n",
       "995756       0.596025            val          train          train   \n",
       "995757       2.564700          train            val          train   \n",
       "995758       3.122500          train          train            val   \n",
       "\n",
       "       split_random_4 split_random_5  weight_lowscores  \n",
       "0               train          train          0.526316  \n",
       "1                 val          train          0.526316  \n",
       "2               train          train          0.526316  \n",
       "3               train            val          0.526316  \n",
       "4                 val          train          0.526316  \n",
       "...               ...            ...               ...  \n",
       "995754          train          train          0.526316  \n",
       "995755          train            val          0.526316  \n",
       "995756          train          train          0.526316  \n",
       "995757          train          train          0.526316  \n",
       "995758          train          train          0.526316  \n",
       "\n",
       "[995759 rows x 9 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
